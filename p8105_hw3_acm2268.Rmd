---
title: "p8105_hw3_acm2268"
author: "Amanda Miles"
date: "10/16/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

#Question 1

##Reading in data

```{r p1_reading_in_data}

library(p8105.datasets)
data("instacart")

```

##Initial Data Exploration

```{r p1_understanding_data}

skimr::skim(instacart)

str(instacart)

```

This dataset is the Instacart Online Grocery Shopping Dataset from 2017 and the file is loaded from p8105.datasets. There are `r nrow(instacart)` rows of data and `r ncol(instacart)` variables in the dataset. The specific variables included are `r names(instacart)`.

There are `r max(instacart$aisle_id)` aisles and the most items are ordered from aisles `r  names(sort(table(instacart$aisle_id), decreasing = TRUE)[1:3])`.

##Plotting the number of items ordered per aisle among aisles with > 10,000 items ordered

```{r p1_plot_n_items_ordered}

n_items_df = instacart %>% 
  group_by(aisle, department) %>%
  summarise(n = n())

filter(n_items_df, n > 10000) %>%
print()

ggplot(n_items_df, aes(x = aisle, y = n, color = department)) + 
  geom_point(alpha = .5) +
  facet_grid(. ~ department)

```

##Creating Table with the 3 Most Popular Items Ordered from the Baking Ingredients, Dog Food Care, and Packaged Vegetables Aisles

```{r p1_table_3_most_popular items}

instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  select(aisle, product_name) %>%
  group_by(product_name) %>%
  mutate(
    n_ordered = n(),
    n_rank = min_rank(desc(n_ordered)))
```

##Table: Mean hour at which Pink Lady Apples and Coffee Ice Cream are ordered each day of the week

```{r p1_mean_hour_dow}

instacart %>% 
  mutate(
    order_dow = recode(order_dow, `0` = "Monday", `1` = "Tuesday", `2` = "Wednesday", `3` = "Thursday", `4` = "Friday", `5` = "Saturday", `6` = "Sunday")) %>%
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(
    mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour)
```

#Question 2: BRFSS

##Load BRFSS data

```{r p2_load_data}

library(p8105.datasets)
data("brfss_smart2010") 

```

##Understand BRFSS data

```{r p2_understand_data}

skimr::skim(brfss_smart2010)

str(brfss_smart2010)

```

##Clean BRFSS data

```{r p2_clean_data}

brfss_df = select(brfss_smart2010, year = Year, state_abbr = Locationabbr, location_desc = Locationdesc, topic = Topic, response = Response, data_value = Data_value, data_type = Data_value_type, data_unit = Data_value_unit ) %>%
  filter(topic == "Overall Health" & response %in% c("Excellent", "Very Good", "Good", "Fair", "Poor")) %>%
  mutate(
    response = factor(response),
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very Good", "Excellent"))
  ) 

```


##BRFSS exploratory data analysis

###Exploratory analysis of states and the number of locations observed in 2002 and 2010

```{r p2_states_n_locs}

brfss_df %>%
 filter(year == 2002) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

brfss_df %>%
 filter(year == 2010) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

```

In 2002, the states Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania were observed at 7 or more locations. In 2010, the states California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas, and Washington were observed at 7 or more locations.

###Exploratory analysis of "excellent" responses

```{r p2_excellent_resp}

excellent_df = select(brfss_df, year, state_abbr, data_value, location_desc, response) %>%
  filter(response == "Excellent") %>%
  group_by(year, state_abbr) %>%
  mutate(avg_value = mean(data_value, na.rm = TRUE)) %>%
  select(-data_value) 

ggplot(data = excellent_df, aes(x = year, y = avg_value, color = state_abbr)) +
  geom_line(data = excellent_df) + theme(legend.position = "right")
  
```

###Exploratory analysis of the distribution of data value responses among locations in NY State FOR 2006 and 2010

```{r}

ny_df = select(brfss_df, year, state_abbr, location_desc, response, data_value) %>%
  filter(year %in% c(2006, 2010) & state_abbr == "NY") %>%
  group_by(year, response)

ggplot(ny_df, aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4, adjust = .5, color = "blue") +
  facet_grid(. ~ year)

ny_df %>% 
  ungroup(response)

ggplot(ny_df, aes(x = data_value, fill = state_abbr)) + 
  geom_density(alpha = .4, adjust = .5, color = "blue") +
  facet_grid(. ~ year)

```


#Question 3: Accelerometer

##Load, clean, and tidy the accelerometer data

```{r p3_load_clean_tidy}

accel_df = read_csv(file = "./data/accel_data.csv") %>%
janitor::clean_names() %>%
  mutate(
    type_of_day = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "weekday", 
                         ifelse(day %in% c("Saturday", "Sunday"), "weekend", ""))
    ) %>%
  relocate(week, type_of_day)

str(accel_df)

```



