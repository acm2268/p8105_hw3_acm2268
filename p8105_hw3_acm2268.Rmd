---
title: "p8105_hw3_acm2268"
author: "Amanda Miles"
date: "10/16/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

#Question 1

##Reading in data

```{r p1_reading_in_data}

library(p8105.datasets)
data("instacart")

```

##Initial Data Exploration

```{r p1_understanding_data}

skimr::skim(instacart)

str(instacart)

```

This dataset is the Instacart Online Grocery Shopping Dataset from 2017 and the file is loaded from p8105.datasets. There are `r nrow(instacart)` rows of data and `r ncol(instacart)` variables in the dataset. The specific variables included are `r names(instacart)`.

There are `r max(instacart$aisle_id)` aisles and the most items are ordered from aisles `r  names(sort(table(instacart$aisle_id), decreasing = TRUE)[1:3])`.

##Plotting the number of items ordered per aisle among aisles with > 10,000 items ordered

```{r p1_plot_n_items_ordered}

n_items_df = instacart %>% 
  group_by(aisle, department) %>%
  summarize(n = n()) %>%
  mutate(
    aisle = factor(aisle),
  ) %>%
  filter(n > 10000)

ggplot(n_items_df, aes(x = reorder(aisle, -n), y = n)) + 
  geom_bar(stat = "identity", fill = "lightblue2") +
  theme(axis.text.x = element_text(family = "Times New Roman", size = 9, angle = 60, hjust = 1)) +
  labs(x = "Aisle Name", y = "Number Ordered", title = "Number of Items Ordered per Aisle")

```

##Creating Table with the 3 Most Popular Items Ordered from the Baking Ingredients, Dog Food Care, and Packaged Vegetables Aisles

```{r p1_table_3_most_popular items}

pop_items_df = instacart %>%
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(
    popular_rank = min_rank(desc(n))) %>%
    filter(popular_rank <= 3) %>%
  arrange(aisle, popular_rank)
  
print(pop_items_df)

```

##Table: Mean hour at which Pink Lady Apples and Coffee Ice Cream are ordered each day of the week

```{r p1_mean_hour_dow}

instacart %>% 
  mutate(
    order_dow = recode(order_dow, `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", `4` = "Thursday", `5` = "Friday", `6` = "Saturday")) %>%
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(
    mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour)
```

#Question 2: BRFSS

##Load BRFSS data

```{r p2_load_data}

library(p8105.datasets)
data("brfss_smart2010") 

```

##Understand BRFSS data

```{r p2_understand_data}

skimr::skim(brfss_smart2010)

str(brfss_smart2010)

```

##Clean BRFSS data

```{r p2_clean_data}

brfss_df = select(brfss_smart2010, year = Year, state_abbr = Locationabbr, location_desc = Locationdesc, topic = Topic, response = Response, data_value = Data_value, data_type = Data_value_type, data_unit = Data_value_unit ) %>%
  filter(topic == "Overall Health" & response %in% c("Excellent", "Good", "Fair", "Poor")) %>%
  mutate(
    response = factor(response),
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Excellent"))
  ) 

```


##BRFSS exploratory data analysis

###Exploratory analysis of states and the number of locations observed in 2002 and 2010

```{r p2_states_n_locs}

brfss_df %>%
 filter(year == 2002) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

brfss_df %>%
 filter(year == 2010) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

```

In 2002, the states Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania were observed at 7 or more locations. In 2010, the states California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas, and Washington were observed at 7 or more locations.

###Exploratory analysis of "excellent" responses

```{r p2_excellent_resp}

excellent_df = select(brfss_df, year, state_abbr, data_value, location_desc, response) %>%
  filter(response == "Excellent") %>%
  group_by(year, state_abbr) %>%
  mutate(avg_value = mean(data_value, na.rm = TRUE)) %>%
  select(-data_value) 

ggplot(data = excellent_df, aes(x = year, y = avg_value, color = state_abbr)) +
  geom_line(data = excellent_df) + 
  theme(
    plot.title = element_text(size = 11),
    axis.title.x = element_text(size = 9),
    axis.title.y = element_text(size = 9),
    legend.position = "right") +
  labs(x = "Year", y = "Average Data Value", title = "Average Data Value of Excellent Responses by State")
  
```

###Exploratory analysis of the distribution of data value responses among locations in NY State FOR 2006 and 2010

```{r}

ny_df = select(brfss_df, year, state_abbr, location_desc, response, data_value) %>%
  filter(year %in% c(2006, 2010) & state_abbr == "NY") %>%
  group_by(year, response)

ggplot(ny_df, aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4, adjust = .5, color = "blue") +
  facet_grid(. ~ year) +
  labs(x = "Data Values", y = "Density", title = "Distribution of Data Values by Response in NY State")

```


#Question 3: Accelerometer

##Load, clean, and tidy the accelerometer data

```{r p3_load_clean_tidy}

accel_df = read_csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names()
  
accel_cleaned = accel_df %>%
  mutate(
    type_of_day = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "weekday", 
                         ifelse(day %in% c("Saturday", "Sunday"), "weekend", ""))
    ) %>%
  relocate(week, type_of_day) %>%
  pivot_longer(
    cols = activity_1:activity_1440, 
    names_to = "activity_number",
    names_prefix = "activity_",
    values_to = "activity_count") %>%
  mutate(
    activity_number = as.numeric(activity_number)
  )

view(accel_cleaned)

```

##Aggregate across minutes to create a total activity variable

```{r p3_total_activity}
accel_cleaned %>% 
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_count))

```
##Create a plot that shows the 24-hour activity time courses for each day

```{r p3_24h_plot}

accel_cleaned %>%
  group_by(day, activity_number) %>%
  ggplot(aes(x = activity_number, 
             y = activity_count, 
             color = day)) +
  geom_smooth(se = FALSE) + 
  theme(legend.position = "right") +
  labs(x = "Minutes", y = "Activity Counts", title = "24-hour Activity by Minute per Day")


```







