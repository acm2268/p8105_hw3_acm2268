---
title: "p8105_hw3_acm2268"
author: "Amanda Miles"
date: "10/16/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

#Question 1

##Reading in data

```{r p1_reading_in_data}

library(p8105.datasets)
data("instacart")

```

##Initial Data Exploration

```{r p1_understanding_data}

skimr::skim(instacart)

str(instacart)

```

This dataset is the Instacart Online Grocery Shopping Dataset from 2017 and the file is loaded from p8105.datasets. There are `r nrow(instacart)` rows of data and `r ncol(instacart)` variables in the dataset. The specific variables included are `r names(instacart)`.

There are `r max(instacart$aisle_id)` aisles and the most items are ordered from aisles `r  names(sort(table(instacart$aisle_id), decreasing = TRUE)[1:3])`.

##Plotting the number of items ordered per aisle among aisles with > 10,000 items ordered

```{r p1_plot_n_items_ordered}

n_items_df = instacart %>% 
  group_by(aisle, department) %>%
  summarize(n = n()) %>%
  mutate(
    aisle = factor(aisle),
  ) %>%
  filter(n > 10000)

ggplot(n_items_df, aes(x = reorder(aisle, -n), y = n)) + 
  geom_bar(stat = "identity", fill = "lightblue2") +
  theme(
    axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
    plot.title = element_text(size = 11)) +
  labs(x = "Aisle Name", y = "Number Ordered", title = "Number of Items Ordered per Aisle, for Aisles with > 10,000 Items Ordered")

```
The plot shows the number of items ordered per aisle for aisles with more than 10,000 items ordered. It shows that the aisles with the greatest number of items ordered are the fresh vegetables aisle and the fresh fruits aisle, followed by the packaged vegetables and fruits aisle. An additional note is that the graph has a strong right-skew. 

##Creating Table with the 3 Most Popular Items Ordered from the Baking Ingredients, Dog Food Care, and Packaged Vegetables Aisles

```{r p1_table_3_most_popular items}

pop_items_df = instacart %>%
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(
    popular_rank = min_rank(desc(n))) %>%
    filter(popular_rank <= 3) %>%
  arrange(aisle, popular_rank)
  
knitr::kable(pop_items_df, 
             digits = 1,
             caption = "*3 Most Popular Items Ordered*")

```

The table shows the three most popular items determined by number of orders in the dog food care aisle, the baking ingredients aisle, and the packaged vegetables fruits aisle. The three most popular items from the packaged vegetables fruits aisle are all organic, with the most popular item being organic baby spinach. The three most popular items from the dog food care aisle are all organic, with the most popular item being Chicken & Vegetable Dog Food. The most popular item from the baking ingredients aisle is light brown sugar. 

While the table shows the three most popular items for those three aisles, the n column shows that there is substantial variability between the three aisles in the number of times the most popular items were ordered. 

##Table: Mean hour at which Pink Lady Apples and Coffee Ice Cream are ordered each day of the week

```{r p1_mean_hour_dow}

instacart_hour =
  instacart %>% 
  mutate(
    order_dow = recode(order_dow, `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", `4` = "Thursday", `5` = "Friday", `6` = "Saturday"),
    order_dow = factor(order_dow)) %>%
  factor(order_dow, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")) %>%
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(
    mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour)
  
knitr::kable(instacart_hour, 
             digits = 1,
             caption = "*Mean Hour Pink Lady Apples and Coffee Ice Cream are Ordered by Day*")


```

The table shows the mean hour that pink lady apples and coffee ice cream are ordered by day of the week. Pink lady apples are generally ordered earlier in the day for all days of the week other than Friday. 


#Question 2: BRFSS

##Load BRFSS data

```{r p2_load_data}

library(p8105.datasets)
data("brfss_smart2010") 

```

##Understand BRFSS data

```{r p2_understand_data}

skimr::skim(brfss_smart2010)

str(brfss_smart2010)

```

##Clean BRFSS data

```{r p2_clean_data}

brfss_df = select(brfss_smart2010, year = Year, state_abbr = Locationabbr, location_desc = Locationdesc, topic = Topic, response = Response, data_value = Data_value, data_type = Data_value_type, data_unit = Data_value_unit ) %>%
  filter(topic == "Overall Health" & response %in% c("Excellent", "Good", "Fair", "Poor")) %>%
  mutate(
    response = factor(response),
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Excellent"))
  ) 

```


##BRFSS exploratory data analysis

###Exploratory analysis of states and the number of locations observed in 2002 and 2010

```{r p2_states_n_locs}

brfss_02_df = brfss_df %>%
 filter(year == 2002) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

knitr::kable(brfss_02_df, 
             digits = 1,
             caption = "*The Number of Locations for States Observed at > 6 Locations in 2002*")

brfss_10_df = 
  brfss_df %>%
 filter(year == 2010) %>%
  group_by(state_abbr) %>%
  summarize(n_locations = n_distinct(location_desc)) %>%
  filter(n_locations > 6)

knitr::kable(brfss_10_df, 
             digits = 1,
             caption = "*The Number of Locations for States Observed at > 6 Locations in 2010*")

```

In 2002, the states Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania were observed at 7 or more locations. In 2010, the states California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas, and Washington were observed at 7 or more locations.

###Exploratory analysis of "excellent" responses

```{r p2_excellent_resp}

excellent_df = select(brfss_df, year, state_abbr, data_value, location_desc, response) %>%
  filter(response == "Excellent") %>%
  group_by(year, state_abbr) %>%
  mutate(avg_value = mean(data_value, na.rm = TRUE)) %>%
  select(-data_value) 

ggplot(data = excellent_df, aes(x = year, y = avg_value, color = state_abbr)) +
  geom_line(data = excellent_df) + 
  theme(
    plot.title = element_text(size = 11),
    axis.title.x = element_text(size = 9),
    axis.title.y = element_text(size = 9),
    legend.position = "right") +
  labs(x = "Year", y = "Average Data Value", title = "Average Data Value of Excellent Responses by State")
  
```

###Exploratory analysis of the distribution of data value responses among locations in NY State FOR 2006 and 2010

```{r}

ny_df = select(brfss_df, year, state_abbr, location_desc, response, data_value) %>%
  filter(year %in% c(2006, 2010) & state_abbr == "NY") %>%
  group_by(year, response)

ggplot(ny_df, aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4, adjust = .5, color = "blue") +
  facet_grid(. ~ year) +
  labs(x = "Data Values", y = "Density", title = "Distribution of Data Values by Response in NY State")

```


#Question 3: Accelerometer

##Load, clean, and tidy the accelerometer data

```{r p3_load_clean_tidy}

accel_df = read_csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names()
  
accel_cleaned = accel_df %>%
  mutate(
    type_of_day = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "weekday", 
                         ifelse(day %in% c("Saturday", "Sunday"), "weekend", ""))
    ) %>%
  relocate(week, type_of_day) %>%
  pivot_longer(
    cols = activity_1:activity_1440, 
    names_to = "activity_number",
    names_prefix = "activity_",
    values_to = "activity_count") %>%
  mutate(
    activity_number = as.numeric(activity_number)
  )

view(accel_cleaned)

```

##Aggregate across minutes to create a total activity variable

```{r p3_total_activity}
accel_tot_act = accel_cleaned %>% 
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_count))

knitr::kable(accel_tot_act, 
             digits = 2,
             caption = "*Total Accelerometer Activity per Day Observed*")

```
##Create a plot that shows the 24-hour activity time courses for each day

```{r p3_24h_plot}

accel_cleaned %>%
  group_by(day, activity_number) %>%
  ggplot(aes(x = activity_number, 
             y = activity_count, 
             color = day)) +
  geom_smooth(se = FALSE) + 
  theme(legend.position = "right") +
  labs(x = "Minutes", y = "Activity Counts", title = "24-hour Activity by Minute per Day")


```







